{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b95e62",
   "metadata": {},
   "source": [
    "# Scraping & Cleaning for Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf24ed",
   "metadata": {},
   "source": [
    "## Import Statements & Functions\n",
    "The import statements and functions we'll call in the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25d30540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b49c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape wikipedia page\n",
    "def scrape(url):\n",
    "    # need header to scrape from wikipedia\n",
    "    headers = {\n",
    "        'User-Agent': 'SchoolProjectBot/1.0 (r1043412@student.thomasmore.be) Python-Requests/2.31.0'\n",
    "    }\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    # short wait\n",
    "    time.sleep(3)\n",
    "    \n",
    "    return BS(page.text, \"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f4088b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take information from BeautifulSoup and clean it\n",
    "def extract_clean_data(soup):\n",
    "    # extract the table from the page html\n",
    "    table = soup.find('table', class_ = 'wikitable')\n",
    "\n",
    "    # use list comprehension instead of for loop\n",
    "    courses = [\n",
    "        # take all td's and put them in a list 'cells', take the second and third elements from that list\n",
    "        (lambda cells: (cells[1].get_text(strip=True), cells[2].get_text(strip=True)))(row.find_all(\"td\"))\n",
    "\n",
    "        # take all rows exept title and results row\n",
    "        for row in table.select('tr')[1:-1]\n",
    "\n",
    "        # only read rows that have more than 2 elements\n",
    "        if len(row.find_all('td')) > 2\n",
    "    ]\n",
    "\n",
    "    # remove rest days, transfer days and remove courses without 'to' as these are very short outliers within a city\n",
    "    courses = [course for course in courses if (re.search('to[A-Z ]', course[0]) and course[1])]\n",
    "\n",
    "    # clean data so we can later perform analisys on it\n",
    "    courses = [\n",
    "        (lambda course:\n",
    "        (course[0][:re.search('to[A-Z ]', course[0]).start()], # start of course\n",
    "        course[0][re.search('to[A-Z ]', course[0]).end()-1:], # end of course\n",
    "        course[1][:re.search(r'\\D', course[1]).start()]) # distance of course\n",
    "        )(course)\n",
    "        for course in courses\n",
    "    ]\n",
    "\n",
    "    # transfer list of cleaned data to a dictionary so it can be written to json\n",
    "    data_dict = [\n",
    "        {'key': i, 'start': course[0], 'end': course[1], 'distance': int(course[2])} # distance must be int to use NumPy on it\n",
    "        \n",
    "        # use enumerate for the counter\n",
    "        for i, course in enumerate(courses, start=1)\n",
    "    ]\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5e271b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to json\n",
    "def write_json(dict, course_year, course_type):\n",
    "    # open the json file and update it with the python dictionary\n",
    "    with open(f'data/{course_type}.json', 'r+') as file:\n",
    "        # load the entire json file into memory\n",
    "        file_json = json.load(file)\n",
    "\n",
    "        # update the json with the python dictionary\n",
    "        file_json[course_type].update({course_year: dict})\n",
    "\n",
    "        # put cursor at beginning of file and overwrite it with the dictionary in memory\n",
    "        file.seek(0)\n",
    "        json.dump(file_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6714e82",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Call the other functions in one function so we can easily feed the grand tours into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f132ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine functions into one functions we can use to scrape and clean the data\n",
    "def scrape_and_clean(grand_tour, start_year):\n",
    "    # set year to start year initially, after which it'll be updated\n",
    "    year = start_year\n",
    "    \n",
    "    # select last part of link and json file based on which grand tour\n",
    "    match grand_tour:\n",
    "        case 'Tour de France':\n",
    "            page_url = '_Tour_de_France'\n",
    "            json_file = 'tour_de_france'\n",
    "        case 'Giro d\\'Italia':\n",
    "            page_url = '_Giro_d\\'Italia'\n",
    "            json_file = 'giro_ditalia'\n",
    "        case 'Vuelta a España':\n",
    "            page_url = '_Vuelta_a_España'\n",
    "            json_file = 'vuelta_a_espagna'\n",
    "\n",
    "    # scrape a page every 5 years\n",
    "    while year < 2025:\n",
    "        # assemble the url and scrape the page\n",
    "        assembled_url = 'https://en.wikipedia.org/wiki/' + str(year) + page_url\n",
    "        page_html = scrape(assembled_url)\n",
    "\n",
    "        # skip the year 2016 for the Giro as it's layout is slightly different, and writing extra code for a singular year isn't worth it\n",
    "        if not ((year == 2016 and grand_tour == 'Giro d\\'Italia') or (year == 2022 and grand_tour == 'Tour de France')):\n",
    "            # test if the table we'll pull is correct -- the page needs to have a table with the class wikitable\n",
    "            if not (page_html.find('table', class_='wikitable') == None):\n",
    "                # check if the wikitable has the table headers of the collumns we want -- check if text exists and if it's correct\n",
    "                if not (page_html.find('table', class_='wikitable').find('th', string=lambda text: text and 'Course' in text) == None \n",
    "                        and page_html.find('table', class_='wikitable').find('th', string=lambda text: text and 'Distance' in text) == None):\n",
    "                    # when the table is correct, carry on with the main logic\n",
    "                    course_dictionary = extract_clean_data(page_html)\n",
    "                    write_json(course_dictionary, year, json_file)\n",
    "                    \n",
    "                    # confirm the scrape of that year was succesful\n",
    "                    print(f'Succesfully scraped and cleaned data of year {year} for {grand_tour}.')\n",
    "\n",
    "                    # add 5 to year to move on to the next wiki page\n",
    "                    year += 5\n",
    "                else:\n",
    "                    # print a message of scrape wasn't sucessful\n",
    "                    print(f'Correct collums not found for {grand_tour} in year {year}.')\n",
    "\n",
    "                    # when the table isn't correct , we'll only add 1 year\n",
    "                    year += 1\n",
    "            else:\n",
    "                # print a message of scrape wasn't sucessful\n",
    "                print(f'Page not found for {grand_tour} in year {year}.')\n",
    "\n",
    "                # when the table isn't correct , we'll only add 1 year\n",
    "                year += 1\n",
    "        else:\n",
    "            print('Skipped tour')\n",
    "            year += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d53aef",
   "metadata": {},
   "source": [
    "## Execution of the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9e1f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct collums not found for Tour de France in year 1905.\n",
      "Succesfully scraped and cleaned data of year 1906 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1911 for Tour de France.\n",
      "Page not found for Tour de France in year 1916.\n",
      "Page not found for Tour de France in year 1917.\n",
      "Page not found for Tour de France in year 1918.\n",
      "Succesfully scraped and cleaned data of year 1919 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1924 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1929 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1934 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1939 for Tour de France.\n",
      "Page not found for Tour de France in year 1944.\n",
      "Page not found for Tour de France in year 1945.\n",
      "Page not found for Tour de France in year 1946.\n",
      "Succesfully scraped and cleaned data of year 1947 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1952 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1957 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1962 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1967 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1972 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1977 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1982 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1987 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1992 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1997 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 2002 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 2007 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 2012 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 2017 for Tour de France.\n",
      "Skipped tour\n",
      "Succesfully scraped and cleaned data of year 2023 for Tour de France.\n",
      "Succesfully scraped and cleaned data of year 1909 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1914 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1919 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1924 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1929 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1934 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1939 for Giro d'Italia.\n",
      "Page not found for Giro d'Italia in year 1944.\n",
      "Page not found for Giro d'Italia in year 1945.\n",
      "Succesfully scraped and cleaned data of year 1946 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1951 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1956 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1961 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1966 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1971 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1976 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1981 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1986 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1991 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1996 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 2001 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 2006 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 2011 for Giro d'Italia.\n",
      "Skipped tour\n",
      "Succesfully scraped and cleaned data of year 2017 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 2022 for Giro d'Italia.\n",
      "Succesfully scraped and cleaned data of year 1935 for Vuelta a España.\n",
      "Page not found for Vuelta a España in year 1940.\n",
      "Succesfully scraped and cleaned data of year 1941 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1946 for Vuelta a España.\n",
      "Page not found for Vuelta a España in year 1951.\n",
      "Page not found for Vuelta a España in year 1952.\n",
      "Page not found for Vuelta a España in year 1953.\n",
      "Page not found for Vuelta a España in year 1954.\n",
      "Succesfully scraped and cleaned data of year 1955 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1960 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1965 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1970 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1975 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1980 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1985 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1990 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 1995 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 2000 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 2005 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 2010 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 2015 for Vuelta a España.\n",
      "Succesfully scraped and cleaned data of year 2020 for Vuelta a España.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally we'll execute the main loop\n",
    "grand_tour_list = [\n",
    "    ('Tour de France', 1905),\n",
    "    ('Giro d\\'Italia', 1909),\n",
    "    ('Vuelta a España', 1935)\n",
    "]\n",
    "\n",
    "[scrape_and_clean(tour, year) for tour, year in grand_tour_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
