{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d30540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b49c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape wikipedia page\n",
    "def scrape(url):\n",
    "    # need header to scrape from wikipedia\n",
    "    headers = {\n",
    "        'User-Agent': 'SchoolProjectBot/1.0 (r1043412@student.thomasmore.be) Python-Requests/2.31.0'\n",
    "    }\n",
    "    page = requests.get(url, headers=headers)\n",
    "    \n",
    "    return BS(page.text, \"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4088b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take information from BeautifulSoup and clean it\n",
    "def extract_clean_data(soup):\n",
    "    # extract the table from the page html\n",
    "    table = soup.find('table', class_ = 'wikitable')\n",
    "\n",
    "    # use list comprehension instead of for loop\n",
    "    courses = [\n",
    "        # take all td's and put them in a list 'cells', take the second and third elements from that list\n",
    "        (lambda cells: (cells[1].get_text(strip=True), cells[2].get_text(strip=True)))(row.find_all(\"td\"))\n",
    "\n",
    "        # take all rows exept title and results row\n",
    "        for row in table.select('tr')[1:-1]\n",
    "    ]\n",
    "\n",
    "    # clean data so we can later perform analisys on it\n",
    "    courses = [\n",
    "        (lambda course:\n",
    "        (course[0][:re.search('to[A-Z]', course[0]).start()], # start of course\n",
    "        course[0][re.search('to[A-Z]', course[0]).end()-1:], # end of course\n",
    "        course[1][:re.search(r'\\D', course[1]).start()]) # distance of course\n",
    "        )(course)\n",
    "        for course in courses\n",
    "    ]\n",
    "\n",
    "    # transfer list of cleaned data to a dictionary so it can be written to json\n",
    "    data_dict = [\n",
    "        {'key': i, 'start': course[0], 'end': course[1], 'distance': course[2]}\n",
    "        \n",
    "        # use enumerate for the counter\n",
    "        for i, course in enumerate(courses, start=1)\n",
    "    ]\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e271b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to json\n",
    "def write_json(dict, course_year, course_type):\n",
    "    # open the json file and update it with the python dictionary\n",
    "    with open(f'data/{course_type}.json', 'r+') as file:\n",
    "        # load the entire json file into memory\n",
    "        file_json = json.load(file)\n",
    "\n",
    "        # update the json with the python dictionary\n",
    "        file_json[course_type].update({course_year: dict})\n",
    "\n",
    "        # put cursor at beginning of file and overwrite it with the dictionary in memory\n",
    "        file.seek(0)\n",
    "        json.dump(file_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f132ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine functions into one functions we can use to scrape and clean the data\n",
    "def scrape_and_clean(grand_tour, start_year):\n",
    "    # set year to start year initially, after which it'll be updated\n",
    "    year = start_year\n",
    "    \n",
    "    # select last part of link and json file based on which grand tour\n",
    "    match grand_tour:\n",
    "        case 'Tour de France':\n",
    "            page_url = '_Tour_de_France'\n",
    "            json_file = 'tour_de_france.json'\n",
    "        case 'Giro d\\'Italia':\n",
    "            page_url = '_Giro_d\\'Italia'\n",
    "            json_file = 'giro_ditalia.json'\n",
    "        case 'Vuelta a Espa単a':\n",
    "            page_url = '_Vuelta_a_Espa単a'\n",
    "            json_file = 'vuelta_a_espagna'\n",
    "\n",
    "    # scrape a page every 5 years\n",
    "    while year > 2025:\n",
    "        # assemble the url and scrape the page\n",
    "        assembled_url = 'https://en.wikipedia.org/wiki/' + year + page_url\n",
    "        page_html = scrape(assembled_url)\n",
    "\n",
    "        # test if the table we'll pull is correct\n",
    "        if not (page_html.find('table', class_='wikitable') == None \n",
    "                or (page_html.find('table', class_='wikitable').find(string='Course') == None \n",
    "                and page_html.find('table', class_='wikitable').find(string='Distance') == None)):\n",
    "            # when the table is correct, carry on with the main logic\n",
    "            course_dictionary = extract_clean_data(page_html)\n",
    "            write_json(course_dictionary, year, json_file)\n",
    "\n",
    "            # add 5 to year to move on to the next wiki page\n",
    "            year += 5\n",
    "        else:\n",
    "            # when the table isn't correct , we'll only add 1 year\n",
    "            year += 1\n",
    "        \n",
    "        # short wait time between each scrape\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1f8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scrape_and_clean() missing 1 required positional argument: 'start_year'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# finally we'll execute the main loop\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mscrape_and_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTour de France\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m scrape_and_clean(\u001b[33m'\u001b[39m\u001b[33mGiro d\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mItalia\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m scrape_and_clean(\u001b[33m'\u001b[39m\u001b[33mVuelta a Espa単a\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: scrape_and_clean() missing 1 required positional argument: 'start_year'"
     ]
    }
   ],
   "source": [
    "# finally we'll execute the main loop\n",
    "scrape_and_clean('Tour de France', 1905)\n",
    "scrape_and_clean('Giro d\\'Italia', 1909)\n",
    "scrape_and_clean('Vuelta a Espa単a', 1935)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
