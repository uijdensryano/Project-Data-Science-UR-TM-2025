{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1790ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    # need header to scrape from wikipedia\n",
    "    headers = {\n",
    "        'User-Agent': 'SchoolProjectBot/1.0 (r1043412@student.thomasmore.be) Python-Requests/2.31.0'\n",
    "    }\n",
    "    page = requests.get(url, headers=headers)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    return BS(page.text, \"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(dict):\n",
    "    with open(f'data/rider_teams.json', 'r+') as file:\n",
    "        # load the entire json file into memory\n",
    "        file_json = json.load(file)\n",
    "\n",
    "        # update the json with the python dictionary\n",
    "        file_json['rider'].append(dict)\n",
    "\n",
    "        # put cursor at beginning of file and overwrite it with the dictionary in memory\n",
    "        file.seek(0)\n",
    "        json.dump(file_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4670389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_links():\n",
    "    soup = scrape('https://www.procyclingstats.com/race/tour-de-france')\n",
    "\n",
    "    team_links = [\n",
    "        team.select('a')['href']\n",
    "\n",
    "        for team in soup.find('ul', class_='topnav').select('li')[2].find('ul').select('li')\n",
    "    ]\n",
    "\n",
    "    return team_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team(team_url):\n",
    "    # scrape the team page first\n",
    "    soup = scrape(team_url)\n",
    "\n",
    "    # extract all rider names and links\n",
    "    # why class_ and not class? TODO\n",
    "    riders = [\n",
    "        {'name': rider.find('div', class_='name').get_text(strip=True), 'link': soup.find('a')['href']}\n",
    "\n",
    "        for rider in soup.find('ul', class_='name').select('li')\n",
    "    ]\n",
    "\n",
    "    for rider, i in enumerate(riders, start=0):\n",
    "        soup = scrape(rider['link'])\n",
    "\n",
    "        rider_teams = {\n",
    "            team.select('div', class_='name').get_text(strip=True)\n",
    "            \n",
    "            for team in soup.find('ul', class_='rdr-teams2').select('li')\n",
    "        }\n",
    "\n",
    "        riders[i].update({'teams': rider_teams})\n",
    "\n",
    "    write_json(riders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_team_links()\n",
    "\n",
    "[scrape_team(link) for link in links]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
